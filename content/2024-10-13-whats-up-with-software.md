+++
title = "What's happening with software?"
date = 2024-10-13T14:33:26+08:00
tags = ["software", "artificial intelligence", "AI", "speculation"]
+++

About 13 years ago, [software was eating the world](https://a16z.com/why-software-is-eating-the-world/), and just last month [taste is eating Silicon Valley](https://www.workingtheorys.com/p/taste-is-eating-silicon-valley). As the author of the post said, ["Taste is eating software. Taste is the new weapon."](https://www.workingtheorys.com/i/149057938/taste-is-eating-software-taste-is-the-new-weapon). While I won't pretend to fully understand what's happening, I can observe what has happened to software and speculate on what it might become.

We have software running on:

* Mainframes, data centers, compute platforms, on-premise servers
* Personal computers and laptops
* Cars, aircraft, drones
* Appliances
* Smartphones, cameras, and other portable devices

As software engineers, we have seen that software is built based on the cultural markets it serves. In some regions, such as Asia or Europe, Apple and Tesla are considered overpriced brands. In other regions, mostly in North America, Apple and Tesla are commonplace and part of the cultural identity.

But if I think about it, that has been the evolution of software since the beginning. It’s not enough to build just functional software; we have to package it as something appealing to individuals and businesses. Taste and culture have been factors in how software gets adopted, regardless of its technical quality.

So what have I observed?

To understand the current state of software evolution, it's crucial to distinguish between two key components: *infrastructure layer* and *presentation layer*. While both are essential, they're evolving at dramatically different rates.

The infrastructure layer has been remarkably resistant to change. This stability is evident across various sectors:

* Financial transactions running on banks or trading platforms will hardly ever change without a drastic push. Do you remember when the Bitcoin whitepaper was published on October 31, 2008?
* Financial technology has been building on top of existing payment infrastructure.

Regardless of what you think about the underlying technology, cryptocurrencies have also been associated with fraud and scams. Because most cryptocurrencies don’t interoperate with the backbone of payment systems, they don’t have enough utility to justify their widespread adoption. It’s not enough to run a decentralized wallet; it has to interoperate. This is just one example drawn from my working experience at a fintech company.

I believe that’s the case for other industries. They are mostly running legacy software that has been working well for many years. It’s very hard to create an incentive to refresh the whole system, and it’s extremely hard to actually work on a new software system that could replace them. There’s a reason why Tesla is still the leader in the automobile software revolution and has only recently been followed by Chinese companies. This is also influenced by the ability of Chinese companies to source components from manufacturers within China and streamline all the software and hardware running on their products.

The presentation layer is where the rapid advancement occurs. When software was eating the world, the iPhone had just come out with iOS. I believe it wasn't until Android became widespread that we started to see smartphones and mobile apps being used by almost everyone, regardless of gender, race, or nationality.

What’s specifically happening in San Francisco is the proliferation of generated user interfaces from language models—what we call AI. ChatGPT’s interactive canvas, Claude’s artifacts, and Cursor are clear examples of the overall trend in how we’re going to interact with software.

It’s the new presentation layer, and as always, the presentation layer is where we can iterate and make changes without causing a lot of damage to society. I know that AI is supposed to be the brain, but more accurately, it’s a detached brain—a mind living outside the body of its host. It can be a ghost, holy spirit, or even a virus, depending on your belief system. That’s my realistic view on the current state of LLMs. What it needs is the capability to interact with APIs, environments, compilers, test suites, and other components reliably and consistently.

The industry leaders in AI have always had big plans for the growth of intelligence, but at the same time, people are still debating what they actually need or want from it. However, I would encourage anyone to take an in-depth look at Dario Amodei's [Machines of Loving Grace](https://darioamodei.com/machines-of-loving-grace). I think it's quite an honest, intriguing, and grandiose vision of what AI will become.

But from the point of view of software engineers and developers, most of the innovation still occurs at the presentation layer only. This includes user interfaces, AR/VR environments, and AI agents that interact with our legacy systems. It takes a major investment push and cultural shift to see it happening elsewhere.
