+++
title = "About Me"
date = 2025-06-12T23:46:00+08:00
path = "about"
+++

A self-taught software engineer from computer and control systems education background.

I am working as a Senior Software Engineer at [Screening Eagle](https://screeningeagle.com/) Singapore.

I have been working on iOS platform since 2008. At that time, I was fascinated and intrigued by the iOS when Apple released the first SDK to developers all over the world with iPhone being hackable and having the same base system as OS X.

Since then, I have worked mostly on mobile applications development on iOS in Swift and Objective-C, and have continued to do so until around 2019 when COVID hits the world. I've explored running various software to interact with cameras or sensors such as LiDAR using ROS on a single board computer such as Raspberry Pi, Jetson, when NVIDIA have just started promoting AI on the edge. However these experimental projects don't produce anything commercially viable.

These explorations, particularly in robotics and the hands-on challenge of building and troubleshooting FPV drones, fundamentally shaped my perspective on software's interaction with physical hardware. I learned that when code directly controls dynamic systems, the stakes are incredibly tangible. Timing and efficiency are critical, and debugging requires a holistic understanding across both software logic and physical phenomena like electrical signals or thermal limits.

Fast forward to 2024, with the release of visionOS, I felt a growing desire to move beyond traditional mobile apps, though I wasn't sure in what direction. Vision Pro is one of a kind, the most powerful "passthrough" headset system built on Apple's vision of augmented reality, and it has the most cleanest user interface framework that I've ever seen from these types of product: *SwiftUI at its best*. However it's not without problems, limitations from security or capabilities point of view. It's very expensive, and clearly it's not something that any business or an individual would invest on unlike the iPhone or even Mac, resulting in less demand to write applications for it.

Coincidentally, this period also saw the significant rise of large language models, particularly after GPT-4's release.

Ever since, I have also dabbled in learning about neural networks, training object detection and image segmentation models using PyTorch, and consequently I had to spend some time to learn more about computer vision, I went through the experience just like everybody else starting on machine learning. A year went by and once again, I realized that the field of machine learning seem to be filled with many academic or research projects that aren't going anywhere. A researcher could work on 10 different projects, probably 1 of them got published as an open source project on github, and that's it. I think that's the reality of research in any field, however fast forward until now, I think there's a paradigm shift after the rise of LLM.

The capabilities of autoregressive transformer to predict the next tokens enhance our capabilities to produce knowledge, as is the case with diffusion models in generating images. This knowledge or impression or images could result in more knowledge, and many ideas and oppportunities opened up. As a result, I feel that I can learn anything that I want to learn, and the only limit is my own context window, credentials, access level, compute power, energy and money. This is where I am right now, and I'm not sure what will unfold in the future.
